{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"heats.txt\", \"r\") as f:\n",
    "        columns = f.readline()\n",
    "        data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has 10 columns: ['\"sbp\"', '\"tobacco\"', '\"ldl\"', '\"adiposity\"', '\"famhist\"', '\"typea\"', '\"obesity\"', '\"alcohol\"', '\"age\"', '\"chd\"']\n"
     ]
    }
   ],
   "source": [
    "columns = columns.strip(\"\\n\").split(\"\\t\")\n",
    "print(\"Data has {} columns: {}\".format(len(columns), columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are totally 462 samples.\n"
     ]
    }
   ],
   "source": [
    "def clean(s):\n",
    "    s = s.strip(\"\\n\").split(\"\\t\")\n",
    "    return s\n",
    "data = list(map(clean, data))\n",
    "data = list(filter(lambda d:len(d)!=1, data))\n",
    "print(\"There are totally {} samples.\".format(len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Change famhist label to int\n",
    "i = 0\n",
    "for d in data:\n",
    "    d[4] = int(d[4] == '\"Present\"')\n",
    "    d = [float(e) for e in d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has shape: (462, 10)\n",
      "Train feature has shape: (400, 9)\n",
      "Train label has shape: (400,)\n",
      "Test feature has shape: (62, 9)\n",
      "Test label has shape: (62,)\n"
     ]
    }
   ],
   "source": [
    "data = np.array(data)\n",
    "train_feature = data[:400, 0:9]\n",
    "train_label = data[:400, 9]\n",
    "test_feature = data[400:, 0:9]\n",
    "test_label = data[400:, 9]\n",
    "print(\"Data has shape: {}\".format(data.shape))\n",
    "print(\"Train feature has shape: {}\".format(train_feature.shape))\n",
    "print(\"Train label has shape: {}\".format(train_label.shape))\n",
    "print(\"Test feature has shape: {}\".format(test_feature.shape))\n",
    "print(\"Test label has shape: {}\".format(test_label.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 9)\n",
      "(256,)\n"
     ]
    }
   ],
   "source": [
    "def next_batch(data, label, batch_size):\n",
    "    data = np.hstack([train_feature, train_label.reshape(400,1)])\n",
    "    np.random.shuffle(data)\n",
    "    feature = data[:batch_size, 0:9]\n",
    "    label = data[:batch_size, 9]\n",
    "    return feature, label\n",
    "next_feature, next_label = next_batch(train_feature, train_label, 256)\n",
    "print(next_feature.shape)\n",
    "print(next_label.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Define parameter      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, 9], name=\"Input\")\n",
    "Y = tf.placeholder(tf.float32, shape=[None], name=\"Label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w1 = tf.Variable(tf.truncated_normal([9, 1], stddev=1.0), name=\"Weights1\")\n",
    "b1 = tf.Variable(tf.zeros([]), name=\"Bias1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define sigmoid loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Loss\"):\n",
    "    logits = tf.matmul(X, w1) + b1\n",
    "    loss = tf.reduce_mean(tf.square(tf.sigmoid(logits) - Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Accuracy\"):\n",
    "    predictions = tf.cast(tf.sigmoid(logits) > 0.5 , tf.float32)\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, Y), tf.float32))\n",
    "    #accuracy, accuracy_op = tf.metrics.accuracy(Y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define summary op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Summaries\"):\n",
    "    loss_summary = tf.summary.scalar(\"loss\", loss)\n",
    "    tf.summary.histogram(\"histogram_loss\", loss)\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n",
      "Loss: 0.6449999809265137 Acc: 0.35499998927116394\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.local_variables_initializer())\n",
    "writer = tf.summary.FileWriter(\"/tmp/graph/lg\", sess.graph)\n",
    "for i in range(200):\n",
    "    next_feature, next_label = next_batch(train_feature, train_label, 400)\n",
    "    _, np_loss, np_accuracy, summary = sess.run([train_op, loss, accuracy, summary_op],\n",
    "                        feed_dict={X:next_feature, Y:next_label})\n",
    "    print(\"Loss: {} Acc: {}\".format(np_loss, np_accuracy))\n",
    "    loss_history.append(np_loss)\n",
    "    writer.add_summary(summary, global_step=i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.29032257199287415\n",
      "[array([[ 0.31422797],\n",
      "       [-0.23275347],\n",
      "       [ 0.35895273],\n",
      "       [-0.61974561],\n",
      "       [ 0.73897272],\n",
      "       [ 0.56570107],\n",
      "       [-0.75406563],\n",
      "       [ 0.54267734],\n",
      "       [-0.09357337]], dtype=float32), -1.6146569e-08]\n",
      "[array([[ 30.83596039],\n",
      "       [ 34.65694046],\n",
      "       [ 48.89940643],\n",
      "       [ 72.17121124],\n",
      "       [ 28.52905273],\n",
      "       [ 34.4697113 ],\n",
      "       [ 32.69628143],\n",
      "       [ 56.41946411],\n",
      "       [ 25.24097824],\n",
      "       [ 25.10131454],\n",
      "       [ 30.10945129],\n",
      "       [ 43.67415237],\n",
      "       [ 43.88537216],\n",
      "       [ 63.63326645],\n",
      "       [ 31.60486603],\n",
      "       [ 63.52596283],\n",
      "       [ 56.58460236],\n",
      "       [ 38.9339447 ],\n",
      "       [ 44.05513   ],\n",
      "       [ 47.18891144],\n",
      "       [ 77.40716553],\n",
      "       [ 42.83596039],\n",
      "       [ 73.65531158],\n",
      "       [ 55.46630096],\n",
      "       [ 37.41217041],\n",
      "       [ 48.84684753],\n",
      "       [ 74.44876099],\n",
      "       [ 51.49710464],\n",
      "       [ 42.18231964],\n",
      "       [ 33.90405273],\n",
      "       [ 48.64043808],\n",
      "       [ 25.82388306],\n",
      "       [ 38.0630455 ],\n",
      "       [ 38.87539291],\n",
      "       [ 59.28488922],\n",
      "       [ 39.60346985],\n",
      "       [ 49.57800674],\n",
      "       [ 42.57430649],\n",
      "       [ 44.60556793],\n",
      "       [ 43.55612183],\n",
      "       [ 26.98429871],\n",
      "       [ 40.28479385],\n",
      "       [ 49.44045258],\n",
      "       [ 43.96657944],\n",
      "       [ 59.52361298],\n",
      "       [ 57.00763702],\n",
      "       [ 35.68362427],\n",
      "       [ 47.04367828],\n",
      "       [ 41.10935974],\n",
      "       [ 86.70516205],\n",
      "       [ 71.4954834 ],\n",
      "       [ 49.58214188],\n",
      "       [ 65.21675873],\n",
      "       [  9.17089462],\n",
      "       [ 43.65534973],\n",
      "       [ 56.59830093],\n",
      "       [ 31.9773407 ],\n",
      "       [ 58.96433258],\n",
      "       [ 51.03681946],\n",
      "       [ 41.15961838],\n",
      "       [ 45.75371552],\n",
      "       [ 42.92584229]], dtype=float32), array([[ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.],\n",
      "       [ 1.]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "# test model\n",
    "test_accuracy = sess.run(accuracy, feed_dict={X:test_feature, Y:test_label})\n",
    "print(\"Test accuracy: {}\".format(test_accuracy))\n",
    "print(sess.run([w1, b1]))\n",
    "print(sess.run([logits,predictions], feed_dict={X:test_feature, Y:test_label}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQpLUybCQJHUyLCRJnQwLSVKn/w9IDPa1kDm1sQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11792d898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_history)\n",
    "plt.title('Loss history')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!tensorboard --logdir=\"/tmp/graph/lg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
